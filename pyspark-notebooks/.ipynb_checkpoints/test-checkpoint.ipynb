{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install -q -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kafka Topic already exists'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"kafka:9092\",\n",
    "    client_id='test'\n",
    ")\n",
    "\n",
    "topic_list = admin_client.list_topics()\n",
    "\n",
    "def create_new_topic():\n",
    "    try:\n",
    "        topic_list = []\n",
    "        topic_list.append(NewTopic(name=\"test\", num_partitions=1, replication_factor=1))\n",
    "        admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
    "        return \"Kafka Topic created\" \n",
    "    except:\n",
    "        return \"Kafka Topic already exists\"\n",
    "\n",
    "create_new_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- order: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- session ID: string (nullable = true)\n",
      " |-- page 1 (main category): string (nullable = true)\n",
      " |-- page 2 (clothing model): string (nullable = true)\n",
      " |-- colour: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- model photography: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- price 2: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      "\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "|year|month|day|order|country|session ID|page 1 (main category)|page 2 (clothing model)|colour|location|model photography|price|price 2|page|\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "|2008|    4|  1|    1|     29|         1|                     1|                    A13|     1|       5|                1|   28|      2|   1|\n",
      "|2008|    4|  1|    2|     29|         1|                     1|                    A16|     1|       6|                1|   33|      2|   1|\n",
      "|2008|    4|  1|    3|     29|         1|                     2|                     B4|    10|       2|                1|   52|      1|   1|\n",
      "|2008|    4|  1|    4|     29|         1|                     2|                    B17|     6|       6|                2|   38|      2|   1|\n",
      "|2008|    4|  1|    5|     29|         1|                     2|                     B8|     4|       3|                2|   52|      1|   1|\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------+---------+---+\n",
      "|  click_id|user_id|timestamp|url|\n",
      "+----------+-------+---------+---+\n",
      "|2008_4_1_1|      1| 2008 4 1|  1|\n",
      "|2008_4_1_2|      1| 2008 4 1|  1|\n",
      "|2008_4_1_3|      1| 2008 4 1|  1|\n",
      "|2008_4_1_4|      1| 2008 4 1|  1|\n",
      "|2008_4_1_5|      1| 2008 4 1|  1|\n",
      "+----------+-------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---+\n",
      "|  click_id|geo|\n",
      "+----------+---+\n",
      "|2008_4_1_1| 29|\n",
      "|2008_4_1_2| 29|\n",
      "|2008_4_1_3| 29|\n",
      "|2008_4_1_4| 29|\n",
      "|2008_4_1_5| 29|\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------+---------+---+---+\n",
      "|  click_id|user_id|timestamp|url|geo|\n",
      "+----------+-------+---------+---+---+\n",
      "|2008_4_1_1|      1| 2008 4 1|  1|  9|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "+----------+-------+---------+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o77.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4525/0x0000000801c6c770.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\n\tat org.apache.spark.sql.Dataset$$Lambda$4188/0x0000000801b5c740.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.Dataset$$Lambda$2063/0x00000008017bc1b0.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset$$Lambda$1726/0x00000008016dd3a8.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1737/0x00000008016e0d78.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1727/0x00000008016dd678.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m final_df_json \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mjson(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/e-shop_clothing_2008.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Convert the DataFrame to a list of dictionaries\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m final_df_list \u001b[38;5;241m=\u001b[39m [row\u001b[38;5;241m.\u001b[39masDict() \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfinal_df_json\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Serialize the list of dictionaries to JSON\u001b[39;00m\n\u001b[1;32m     73\u001b[0m final_df_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(final_df_list)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o77.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:415)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:412)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:449)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:448)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$4525/0x0000000801c6c770.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3997)\n\tat org.apache.spark.sql.Dataset$$Lambda$4188/0x0000000801b5c740.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.Dataset$$Lambda$2063/0x00000008017bc1b0.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset$$Lambda$1726/0x00000008016dd3a8.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1737/0x00000008016e0d78.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$1727/0x00000008016dd678.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3994)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import concat_ws, col, when \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Spark Read Write\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"./data/e-shop_clothing_2008.csv\", header=True, sep=\";\") \n",
    "\n",
    "# Print the schema of the DataFrame \n",
    "df.printSchema() \n",
    "\n",
    "# Show the first 5 rows of the DataFrame \n",
    "df.show(5)\n",
    " \n",
    "\n",
    "# Create a DataFrame with the required schema\n",
    "click_data_df = df.select(\n",
    "    concat_ws(\"_\", col(\"year\"), col(\"month\"), col(\"day\"), col(\"order\")).alias(\"click_id\"),\n",
    "    col(\"session ID\").alias(\"user_id\"),\n",
    "    concat_ws(\" \", col(\"year\"), col(\"month\"), col(\"day\")).alias(\"timestamp\"),\n",
    "    col(\"page\").alias(\"url\")\n",
    ")\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "click_data_df.show(5)\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "country_map = {1: 'Australia', 2: 'Austria', 3: 'Belgium', 4: 'British Virgin Islands', 5: 'Cayman Islands', 6: 'Christmas Island', 7: 'Croatia', 8: 'Cyprus', 9: 'Czech Republic', 10: 'Denmark', 11: 'Estonia', 12: 'unidentified', 13: 'Faroe Islands', 14: 'Finland', 15: 'France', 16: 'Germany', 17: 'Greece', 18: 'Hungary', 19: 'Iceland', 20: 'India', 21: 'Ireland', 22: 'Italy', 23: 'Latvia', 24: 'Lithuania', 25: 'Luxembourg', 26: 'Mexico', 27: 'Netherlands', 28: 'Norway', 29: 'Poland', 30: 'Portugal', 31: 'Romania', 32: 'Russia', 33: 'San Marino', 34: 'Slovakia', 35: 'Slovenia', 36: 'Spain', 37: 'Sweden', 38: 'Switzerland', 39: 'Ukraine', 40: 'United Arab Emirates', 41: 'United Kingdom', 42: 'USA', 43: 'biz (*.biz)', 44: 'com (*.com)', 45: 'int (*.int)', 46: 'net (*.net)', 47: 'org (*.org)'}\n",
    "\n",
    "# get geo_data_df which contains the country name for each click_id \n",
    "geo_data_df = df.select(\n",
    "    concat_ws(\"_\", col(\"year\"), col(\"month\"), col(\"day\"), col(\"order\")).alias(\"click_id\"),\n",
    "    col(\"country\").alias(\"geo\")\n",
    ")\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "geo_data_df.show(5)\n",
    "\n",
    "# Join the two DataFrames on click_id to create a single DataFrame with the required schema\n",
    "final_df = click_data_df.join(geo_data_df, \"click_id\", \"inner\")\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "final_df.show(5)\n",
    "\n",
    "# convert the dataframe to json \n",
    "# final_df_json = final_df.toJSON()\n",
    "\n",
    "# json_rdd = final_df.map(lambda row: json.dumps(json.loads(row), default=str))\n",
    "# final_df_list = [row.asDict() for row in final_df.collect()]\n",
    "# final_df_json = json.dumps(final_df_list)\n",
    "# final_df_json.saveAsTextFile(\"./data/e-shop_clothing_2008.json\")\n",
    "# Set the number of partitions to use when writing to disk\n",
    "num_partitions = 10\n",
    "\n",
    "# Write the DataFrame to disk in small batches\n",
    "final_df.repartition(num_partitions).write.json(\"./data/e-shop_clothing_2008.json\", mode=\"overwrite\")\n",
    "\n",
    "# # Read the JSON file back into a DataFrame\n",
    "# final_df_json = spark.read.json(\"./data/e-shop_clothing_2008.json\")\n",
    "\n",
    "# # Convert the DataFrame to a list of dictionaries\n",
    "# final_df_list = [row.asDict() for row in final_df_json.collect()]\n",
    "\n",
    "# # Serialize the list of dictionaries to JSON\n",
    "# final_df_json = json.dumps(final_df_list)\n",
    "# print(final_df_json.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating topic\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient \n",
    "from kafka.admin import NewTopic \n",
    "from kafka.errors import TopicAlreadyExistsError \n",
    "import ujson\n",
    "import logging \n",
    "import time \n",
    "from typing import Dict, List\n",
    "import json \n",
    "\n",
    "class KafkaAdapter:\n",
    "    def __init__(self, \n",
    "                  value_deserializer = lambda x:ujson.loads(x.decode('utf-16')),\n",
    "                  value_serializer = lambda value: ujson.dumps(value).encode('utf-16')\n",
    "                  ):\n",
    "        self.name = \"KafkaAdapter\"\n",
    "        self.value_deserializer = value_deserializer \n",
    "        self.value_serializer = value_serializer\n",
    "\n",
    "    def produce(self, bootstrap_servers: List, topic_name, data, partition):\n",
    "        producer = KafkaProducer(bootstrap_servers=bootstrap_servers,\n",
    "                                 value_serializer=self.value_serializer,\n",
    "                                 acks=\"all\",\n",
    "                                 retries=3\n",
    "                                 )\n",
    "    \n",
    "        try:\n",
    "            producer.send(topic=topic_name, value=data, partition=partition)\n",
    "            logging.info(f\"Data: {data} Sent to topic: {topic_name} on partition: {partition}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while sending data to topic: {topic_name} on partition: {partition} with error: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            producer.close()\n",
    "\n",
    "    def consume(self, bootstrap_servers: List, topic_name, group_id, auto_offset_reset=\"earliest\", enable_auto_commit=True, customer_timeout=3000):\n",
    "        try:\n",
    "            consumer = KafkaConsumer(\n",
    "                topic_name,\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                group_id=group + topic_name,\n",
    "                value_deserializer=self.value_deserializer,\n",
    "                enable_auto_commit=enable_auto_commit,\n",
    "                auto_offset_reset=auto_offset_reset,\n",
    "                customer_timeout=customer_timeout\n",
    "            )\n",
    "            logging.info(f\"Data consumed from {topic_name}\")\n",
    "            return consumer\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while consuming data from {topic_name} with error: {e}\")\n",
    "\n",
    "    topic_config_list = [{\"topic_name\": \"click_data\", \"num_partitions\": 1, \"replication_factor\": 1}]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_topic(bootstrap_servers: List, topic_config_list:List[Dict]):\n",
    "        client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
    "        topics, topics_list = [], []\n",
    "        for topic_config in topic_config_list:\n",
    "            new_topic = NewTopic(name=topic_config[\"topic_name\"], num_partitions=topic_config[\"num_partitions\"], replication_factor=topic_config[\"replication_factor\"])\n",
    "            topics.append(new_topic)\n",
    "            topics_list.append(topic_config[\"topic_name\"])\n",
    "        try:\n",
    "            client.create_topics(new_topics=topics)\n",
    "            logging.info(f\"Topics {topics_list} created successfully\")\n",
    "        except TopicAlreadyExistsError as e:\n",
    "            client.delete_topics(topics_list, timeout_ms=1000)\n",
    "            time.sleep(1)\n",
    "            client.create_topics(new_topics=topics)\n",
    "            logging.info(f\"Topics {topics_list} deleted and recreated successfully\")\n",
    "\n",
    "    def create_producer(self, bootstrap_servers: List):\n",
    "        try:\n",
    "            producer = KafkaProducer(bootstrap_servers=bootstrap_servers,\n",
    "                                 value_serializer=self.value_serializer,\n",
    "                                 acks=\"all\",\n",
    "                                 retries=3\n",
    "                                 )\n",
    "            return producer\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while creating producer with error: {e}\")\n",
    "\n",
    "adapter = KafkaAdapter() \n",
    "bootstrap_servers = [\"kafka:9092\"]\n",
    "topic_config_list = [{\"topic_name\": \"click_data\", \"num_partitions\": 1, \"replication_factor\": 1}]\n",
    "def init_kafka():\n",
    "    try:\n",
    "        print(\"Creating topic\")\n",
    "        logging.info(\"Creating a topic in kafka\")\n",
    "        adapter.create_topic(bootstrap_servers=bootstrap_servers, topic_config_list=topic_config_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while creating topic with error: {e}\")\n",
    "\n",
    "def send_to_kafka(data):\n",
    "    try:\n",
    "        adapter.produce(bootstrap_servers=bootstrap_servers, topic_name=\"click_data\", directory_location=data, partition= 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while sending data to kafka with error: {e}\")\n",
    "\n",
    "import os\n",
    "\n",
    "def send_to_kafka(data_dir):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark Read Write\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            df = spark.read.json(file_path)\n",
    "            for row in df.collect():\n",
    "                try:\n",
    "                    adapter.produce(\n",
    "                        bootstrap_servers=bootstrap_servers,\n",
    "                        topic_name=\"click_data\",\n",
    "                        data=row.asDict(),\n",
    "                        partition=0\n",
    "                    )\n",
    "                    logging.info(f\"Data: {row} Sent to topic: click_data on partition: 0\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error while sending data to kafka with error: {e}\")\n",
    "\n",
    "                \n",
    "def read_stream_from_kafka():\n",
    "    spark_read = spark \\\n",
    "                    .readStream \\\n",
    "                    .format(\"kafka\") \\\n",
    "                    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "                    .option(\"subscribe\", \"click_data\") \\\n",
    "                    .load()\n",
    "    print(spark_read.isStreaming)\n",
    "\n",
    "    spark_read.printSchema()\n",
    "\n",
    "    spark_read.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "        .select(from_json(col(\"value\"), struct).alias(\"data\"))\n",
    "    \n",
    "    spark_read.printSchema()\n",
    "\n",
    "init_kafka()\n",
    "send_to_kafka(\"./data/e-shop_clothing_2008.json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "admin_client.delete_topics(topics = [\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['click_data']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_client.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
