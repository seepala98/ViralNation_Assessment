{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install -q -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connect-offsets', 'connect-status', 'connect-config', '__consumer_offsets']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kafka Topic created'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"kafka:9092\",\n",
    "    client_id='test'\n",
    ")\n",
    "\n",
    "topic_list = admin_client.list_topics()\n",
    "print(topic_list)\n",
    "\n",
    "def create_new_topic():\n",
    "    try:\n",
    "        topic_list = []\n",
    "        topic_list.append(NewTopic(name=\"test\", num_partitions=1, replication_factor=1))\n",
    "        admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
    "        return \"Kafka Topic created\" \n",
    "    except:\n",
    "        return \"Kafka Topic already exists\"\n",
    "\n",
    "create_new_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'connect-config',\n",
       " 'connect-status',\n",
       " 'connect-offsets',\n",
       " '__consumer_offsets']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_client.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- order: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- session ID: string (nullable = true)\n",
      " |-- page 1 (main category): string (nullable = true)\n",
      " |-- page 2 (clothing model): string (nullable = true)\n",
      " |-- colour: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- model photography: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- price 2: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      "\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "|year|month|day|order|country|session ID|page 1 (main category)|page 2 (clothing model)|colour|location|model photography|price|price 2|page|\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "|2008|    4|  1|    1|     29|         1|                     1|                    A13|     1|       5|                1|   28|      2|   1|\n",
      "|2008|    4|  1|    2|     29|         1|                     1|                    A16|     1|       6|                1|   33|      2|   1|\n",
      "|2008|    4|  1|    3|     29|         1|                     2|                     B4|    10|       2|                1|   52|      1|   1|\n",
      "|2008|    4|  1|    4|     29|         1|                     2|                    B17|     6|       6|                2|   38|      2|   1|\n",
      "|2008|    4|  1|    5|     29|         1|                     2|                     B8|     4|       3|                2|   52|      1|   1|\n",
      "+----+-----+---+-----+-------+----------+----------------------+-----------------------+------+--------+-----------------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------+---------+---+\n",
      "|  click_id|user_id|timestamp|url|\n",
      "+----------+-------+---------+---+\n",
      "|2008_4_1_1|      1| 2008 4 1|  1|\n",
      "|2008_4_1_2|      1| 2008 4 1|  1|\n",
      "|2008_4_1_3|      1| 2008 4 1|  1|\n",
      "|2008_4_1_4|      1| 2008 4 1|  1|\n",
      "|2008_4_1_5|      1| 2008 4 1|  1|\n",
      "+----------+-------+---------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---+\n",
      "|  click_id|geo|\n",
      "+----------+---+\n",
      "|2008_4_1_1| 29|\n",
      "|2008_4_1_2| 29|\n",
      "|2008_4_1_3| 29|\n",
      "|2008_4_1_4| 29|\n",
      "|2008_4_1_5| 29|\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------+---------+---+---+\n",
      "|  click_id|user_id|timestamp|url|geo|\n",
      "+----------+-------+---------+---+---+\n",
      "|2008_4_1_1|      1| 2008 4 1|  1|  9|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "|2008_4_1_1|      1| 2008 4 1|  1| 29|\n",
      "+----------+-------+---------+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import concat_ws, col, when \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Spark Read Write\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"./data/e-shop_clothing_2008.csv\", header=True, sep=\";\") \n",
    "\n",
    "# Print the schema of the DataFrame \n",
    "df.printSchema() \n",
    "\n",
    "# Show the first 5 rows of the DataFrame \n",
    "df.show(5)\n",
    " \n",
    "\n",
    "# Create a DataFrame with the required schema\n",
    "click_data_df = df.select(\n",
    "    concat_ws(\"_\", col(\"year\"), col(\"month\"), col(\"day\"), col(\"order\")).alias(\"click_id\"),\n",
    "    col(\"session ID\").alias(\"user_id\"),\n",
    "    concat_ws(\" \", col(\"year\"), col(\"month\"), col(\"day\")).alias(\"timestamp\"),\n",
    "    col(\"page\").alias(\"url\")\n",
    ")\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "click_data_df.show(5)\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "country_map = {1: 'Australia', 2: 'Austria', 3: 'Belgium', 4: 'British Virgin Islands', 5: 'Cayman Islands', 6: 'Christmas Island', 7: 'Croatia', 8: 'Cyprus', 9: 'Czech Republic', 10: 'Denmark', 11: 'Estonia', 12: 'unidentified', 13: 'Faroe Islands', 14: 'Finland', 15: 'France', 16: 'Germany', 17: 'Greece', 18: 'Hungary', 19: 'Iceland', 20: 'India', 21: 'Ireland', 22: 'Italy', 23: 'Latvia', 24: 'Lithuania', 25: 'Luxembourg', 26: 'Mexico', 27: 'Netherlands', 28: 'Norway', 29: 'Poland', 30: 'Portugal', 31: 'Romania', 32: 'Russia', 33: 'San Marino', 34: 'Slovakia', 35: 'Slovenia', 36: 'Spain', 37: 'Sweden', 38: 'Switzerland', 39: 'Ukraine', 40: 'United Arab Emirates', 41: 'United Kingdom', 42: 'USA', 43: 'biz (*.biz)', 44: 'com (*.com)', 45: 'int (*.int)', 46: 'net (*.net)', 47: 'org (*.org)'}\n",
    "\n",
    "# get geo_data_df which contains the country name for each click_id \n",
    "geo_data_df = df.select(\n",
    "    concat_ws(\"_\", col(\"year\"), col(\"month\"), col(\"day\"), col(\"order\")).alias(\"click_id\"),\n",
    "    col(\"country\").alias(\"geo\")\n",
    ")\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "geo_data_df.show(5)\n",
    "\n",
    "# Join the two DataFrames on click_id to create a single DataFrame with the required schema\n",
    "final_df = click_data_df.join(geo_data_df, \"click_id\", \"inner\")\n",
    "\n",
    "# Show the first 5 rows of the DataFrame\n",
    "final_df.show(5)\n",
    "\n",
    "# convert the dataframe to json \n",
    "# final_df_json = final_df.toJSON()\n",
    "\n",
    "# json_rdd = final_df.map(lambda row: json.dumps(json.loads(row), default=str))\n",
    "# final_df_list = [row.asDict() for row in final_df.collect()]\n",
    "# final_df_json = json.dumps(final_df_list)\n",
    "# final_df_json.saveAsTextFile(\"./data/e-shop_clothing_2008.json\")\n",
    "# Set the number of partitions to use when writing to disk\n",
    "num_partitions = 10\n",
    "\n",
    "# Write the DataFrame to disk in small batches\n",
    "final_df.repartition(num_partitions).write.json(\"./data/e-shop_clothing_2008.json\", mode=\"overwrite\")\n",
    "\n",
    "# # Read the JSON file back into a DataFrame\n",
    "# final_df_json = spark.read.json(\"./data/e-shop_clothing_2008.json\")\n",
    "\n",
    "# # Convert the DataFrame to a list of dictionaries\n",
    "# final_df_list = [row.asDict() for row in final_df_json.collect()]\n",
    "\n",
    "# # Serialize the list of dictionaries to JSON\n",
    "# final_df_json = json.dumps(final_df_list)\n",
    "# print(final_df_json.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating topic\n",
      "this is the file ./data/e-shop_clothing_2008.json/part-00000-3378a02e-3d33-4d9d-b84c-bac17fbf29d3-c000.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 129\u001b[0m\n\u001b[1;32m    124\u001b[0m                     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending data to kafka with error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m init_kafka()\n\u001b[0;32m--> 129\u001b[0m \u001b[43msend_to_kafka\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/e-shop_clothing_2008.json/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 116\u001b[0m, in \u001b[0;36msend_to_kafka\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcollect():\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# print(row)\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m         \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbootstrap_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtopic_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclick_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sent to topic: click_data on partition: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mKafkaAdapter.produce\u001b[0;34m(self, bootstrap_servers, topic_name, data, partition)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, bootstrap_servers: List, topic_name, data, partition):\n\u001b[0;32m---> 20\u001b[0m     producer \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaProducer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbootstrap_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap_servers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue_serializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_serializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43macks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         producer\u001b[38;5;241m.\u001b[39msend(topic\u001b[38;5;241m=\u001b[39mtopic_name, value\u001b[38;5;241m=\u001b[39mdata, partition\u001b[38;5;241m=\u001b[39mpartition)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kafka/producer/kafka.py:381\u001b[0m, in \u001b[0;36mKafkaProducer.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    378\u001b[0m reporters \u001b[38;5;241m=\u001b[39m [reporter() \u001b[38;5;28;01mfor\u001b[39;00m reporter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_reporters\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics \u001b[38;5;241m=\u001b[39m Metrics(metric_config, reporters)\n\u001b[0;32m--> 381\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_group_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproducer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mwakeup_timeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_block_ms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Get auto-discovered version from client if necessary\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kafka/client_async.py:244\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     check_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version_auto_timeout_ms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_timeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kafka/client_async.py:909\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     remaining \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 909\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbootstrap_topics_filter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# cache the api versions map if it's available (starting\u001b[39;00m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;66;03m# in 0.10 cluster version)\u001b[39;00m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_versions \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mget_api_versions()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kafka/conn.py:1243\u001b[0m, in \u001b[0;36mBrokerConnection.check_version\u001b[0;34m(self, timeout, strict, topics)\u001b[0m\n\u001b[1;32m   1241\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request)\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;66;03m# HACK: sleeping to wait for socket to send bytes\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;66;03m# when broker receives an unrecognized request API\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;66;03m# it abruptly closes our socket.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;66;03m# so we attempt to send a second request immediately\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;66;03m# immediately fail and allow us to infer that the prior\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;66;03m# request was unrecognized\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m mr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(MetadataRequest[\u001b[38;5;241m0\u001b[39m](topics))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient \n",
    "from kafka.admin import NewTopic \n",
    "from kafka.errors import TopicAlreadyExistsError \n",
    "import ujson\n",
    "import logging \n",
    "import time \n",
    "from typing import Dict, List\n",
    "import json \n",
    "\n",
    "class KafkaAdapter:\n",
    "    def __init__(self, \n",
    "                  value_deserializer = lambda x:ujson.loads(x.decode('utf-16')),\n",
    "                  value_serializer = lambda value: ujson.dumps(value).encode('utf-16')\n",
    "                  ):\n",
    "        self.name = \"KafkaAdapter\"\n",
    "        self.value_deserializer = value_deserializer \n",
    "        self.value_serializer = value_serializer\n",
    "\n",
    "    def produce(self, bootstrap_servers: List, topic_name, data, partition):\n",
    "        producer = KafkaProducer(bootstrap_servers=bootstrap_servers,\n",
    "                                 value_serializer=self.value_serializer,\n",
    "                                 acks=\"all\",\n",
    "                                 retries=3\n",
    "                                 )\n",
    "    \n",
    "        try:\n",
    "            producer.send(topic=topic_name, value=data, partition=partition)\n",
    "            logging.info(f\"Data: {data} Sent to topic: {topic_name} on partition: {partition}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while sending data to topic: {topic_name} on partition: {partition} with error: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            producer.close()\n",
    "\n",
    "    def consume(self, bootstrap_servers: List, topic_name, group_id, auto_offset_reset=\"earliest\", enable_auto_commit=True, customer_timeout=3000):\n",
    "        try:\n",
    "            consumer = KafkaConsumer(\n",
    "                topic_name,\n",
    "                bootstrap_servers=bootstrap_servers,\n",
    "                group_id=group + topic_name,\n",
    "                value_deserializer=self.value_deserializer,\n",
    "                enable_auto_commit=enable_auto_commit,\n",
    "                auto_offset_reset=auto_offset_reset,\n",
    "                customer_timeout=customer_timeout\n",
    "            )\n",
    "            logging.info(f\"Data consumed from {topic_name}\")\n",
    "            return consumer\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while consuming data from {topic_name} with error: {e}\")\n",
    "\n",
    "    topic_config_list = [{\"topic_name\": \"click_data\", \"num_partitions\": 1, \"replication_factor\": 1}]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_topic(bootstrap_servers: List, topic_config_list:List[Dict]):\n",
    "        client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
    "        topics, topics_list = [], []\n",
    "        for topic_config in topic_config_list:\n",
    "            new_topic = NewTopic(name=topic_config[\"topic_name\"], num_partitions=topic_config[\"num_partitions\"], replication_factor=topic_config[\"replication_factor\"])\n",
    "            topics.append(new_topic)\n",
    "            topics_list.append(topic_config[\"topic_name\"])\n",
    "        try:\n",
    "            client.create_topics(new_topics=topics)\n",
    "            logging.info(f\"Topics {topics_list} created successfully\")\n",
    "        except TopicAlreadyExistsError as e:\n",
    "            client.delete_topics(topics_list, timeout_ms=1000)\n",
    "            time.sleep(1)\n",
    "            client.create_topics(new_topics=topics)\n",
    "            logging.info(f\"Topics {topics_list} deleted and recreated successfully\")\n",
    "\n",
    "    def create_producer(self, bootstrap_servers: List):\n",
    "        try:\n",
    "            producer = KafkaProducer(bootstrap_servers=bootstrap_servers,\n",
    "                                 value_serializer=self.value_serializer,\n",
    "                                 acks=\"all\",\n",
    "                                 retries=3\n",
    "                                 )\n",
    "            return producer\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error while creating producer with error: {e}\")\n",
    "\n",
    "adapter = KafkaAdapter() \n",
    "bootstrap_servers = [\"kafka:9092\"]\n",
    "topic_config_list = [{\"topic_name\": \"click_data\", \"num_partitions\": 1, \"replication_factor\": 1}]\n",
    "def init_kafka():\n",
    "    try:\n",
    "        print(\"Creating topic\")\n",
    "        logging.info(\"Creating a topic in kafka\")\n",
    "        adapter.create_topic(bootstrap_servers=bootstrap_servers, topic_config_list=topic_config_list)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while creating topic with error: {e}\")\n",
    "\n",
    "def send_to_kafka(data):\n",
    "    try:\n",
    "        adapter.produce(bootstrap_servers=bootstrap_servers, topic_name=\"click_data\", directory_location=data, partition= 0)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while sending data to kafka with error: {e}\")\n",
    "\n",
    "import os\n",
    "\n",
    "def send_to_kafka(data_dir):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark Read Write\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            print(f\"this is the file {file_path}\")\n",
    "            df = spark.read.json(file_path)\n",
    "            for row in df.collect():\n",
    "                # print(row)\n",
    "                try:\n",
    "                    adapter.produce(\n",
    "                        bootstrap_servers=bootstrap_servers,\n",
    "                        topic_name=\"click_data\",\n",
    "                        data=row.asDict(),\n",
    "                        partition=0\n",
    "                    )\n",
    "                    logging.info(f\"Data: {row} Sent to topic: click_data on partition: 0\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error while sending data to kafka with error: {e}\")\n",
    "\n",
    "            \n",
    "\n",
    "init_kafka()\n",
    "send_to_kafka(\"./data/e-shop_clothing_2008.json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['click_data',\n",
       " 'test',\n",
       " 'connect-config',\n",
       " 'connect-status',\n",
       " 'connect-offsets',\n",
       " '__consumer_offsets']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_client.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='test', error_code=0)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_client.delete_topics(topics = [\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['click_data',\n",
       " 'connect-config',\n",
       " 'connect-status',\n",
       " 'connect-offsets',\n",
       " '__consumer_offsets']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin_client.list_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, struct\n",
    "\n",
    "spark_read_stream = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def read_stream_from_kafka():    \n",
    "    spark_read = spark_read_stream \\\n",
    "                    .readStream \\\n",
    "                    .format(\"kafka\") \\\n",
    "                    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "                    .option(\"subscribe\", \"click_data\") \\\n",
    "                    .load() \n",
    "                    # .select(from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"parsed_data\"))\n",
    "    \n",
    "    print(spark_read.isStreaming)\n",
    "\n",
    "    spark_read.printSchema()\n",
    "\n",
    "#     spark_read.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "#         .select(from_json(col(\"value\"), struct).alias(\"data\"))\n",
    "    \n",
    "#     spark_read.printSchema()\n",
    "    return spark_read\n",
    "\n",
    "spark_kafka_consume_read  = read_stream_from_kafka()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o228.start.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.elasticsearch.spark.sql. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:367)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 14 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 43\u001b[0m\n\u001b[1;32m     32\u001b[0m     spark_read \\\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.elasticsearch.spark.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes.resource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclick_data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     41\u001b[0m     spark_read\u001b[38;5;241m.\u001b[39mawaitTermination()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mwrite_stream_to_elasticsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_kafka_consume_read\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m, in \u001b[0;36mwrite_stream_to_elasticsearch\u001b[0;34m(spark_read)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_stream_to_elasticsearch\u001b[39m(spark_read): \n\u001b[1;32m     32\u001b[0m     \u001b[43mspark_read\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.elasticsearch.spark.sql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.nodes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43melasticsearch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.port\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m9200\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mes.resource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclick_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     spark_read\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:1385\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o228.start.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: org.elasticsearch.spark.sql. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:367)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:249)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:587)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:520)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 14 more\n"
     ]
    }
   ],
   "source": [
    "# def writeToElasticsearch(df):\n",
    "\n",
    "#     df.write \\\n",
    "#         .format(\"org.elasticsearch.spark.sql\") \\\n",
    "#         .option(\"es.nodes\", \"localhost\") \\\n",
    "#         .option(\"es.port\", \"9200\") \\\n",
    "#         .option(\"es.resource\", \"office-index\") \\\n",
    "#         .mode(\"append\") \\\n",
    "# #         .save()\n",
    "\n",
    "\n",
    "# streamingQuery = spark_kafka_consume_read.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"json\") \\\n",
    "#     .option(\"es.nodes\", \"localhost\") \\\n",
    "#     .option(\"es.port\", \"9200\") \\\n",
    "#     .start()\n",
    "#     # .option(\"checkpointLocation\", checkpointDir) \\\n",
    "#     # .option(\"numRows\", 4) \\\n",
    "#     # .option(\"truncate\", False) \\\n",
    "#     # .start()\n",
    "\n",
    "# streamingQuery.awaitTermination()\n",
    "spark_write_stream = SparkSession.builder \\\n",
    "    .appName(\"MyAppspark_write\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "def write_stream_to_elasticsearch(spark_read): \n",
    "\n",
    "    spark_read \\\n",
    "        .writeStream \\\n",
    "        .format(\"org.elasticsearch.spark.sql\") \\\n",
    "        .option(\"es.nodes\", \"elasticsearch\") \\\n",
    "        .option(\"es.port\", \"9200\") \\\n",
    "        .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "        .option(\"es.resource\", \"click_data\") \\\n",
    "        .start()\n",
    "    \n",
    "    spark_read.awaitTermination()\n",
    "\n",
    "write_stream_to_elasticsearch(spark_kafka_consume_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
